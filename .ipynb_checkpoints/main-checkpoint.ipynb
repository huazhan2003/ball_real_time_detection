{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f080e00",
   "metadata": {},
   "source": [
    "# 样例介绍\n",
    "* YOLOv5是一种单阶段目标检测算法，在这个样例中，我们选取了YOLOv5s，它是YOLOv5系列中较为轻量的网络，适合在边缘设备部署，进行实时目标检测。\n",
    "\n",
    "# 前期准备\n",
    "* 基础镜像的样例目录中已包含转换后的om模型以及测试图片，如果直接运行，可跳过此步骤。如果需要重新转换模型，可以参考下面的步骤。\n",
    "* 首先我们可以在[这个链接](https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Atlas%20200I%20DK%20A2/DevKit/downloads/23.0.RC1/Ascend-devkit_23.0.RC1_downloads.xlsx)的表格中找到本样例的依赖文件，下载我们已经准备好了的ONNX模型，ONNX是开源的离线推理模型框架。\n",
    "\n",
    "* 为了能进一步优化模型推理性能，我们需要将其转换为om模型进行使用，以下为转换指令：  \n",
    "    ```shell\n",
    "    atc --model=yolov5s.onnx --framework=5 --output=yolo --input_format=NCHW --input_shape=\"input_image:1,3,640,640\" --log=error --soc_version=Ascend310B1\n",
    "    ```\n",
    "    * 其中转换参数的含义为：  \n",
    "        * --model：输入模型路径\n",
    "        * --framework：原始网络模型框架类型，5表示ONNX\n",
    "        * --output：输出模型路径\n",
    "        * --input_format：输入Tensor的内存排列方式\n",
    "        * --input_shape：指定模型输入数据的shape\n",
    "        * --log：日志级别\n",
    "        * --soc_version：昇腾AI处理器型号\n",
    "        * --input_fp16_nodes：指定输入数据类型为FP16的输入节点名称\n",
    "        * --output_type：指定网络输出数据类型或指定某个输出节点的输出类型\n",
    "\n",
    "# 模型推理实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7761be-840e-4fc8-b501-7a5f9a0a56b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入代码依赖\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import torch\n",
    "from sklearn.cluster import KMeans\n",
    "from skvideo.io import vreader, FFmpegWriter\n",
    "import IPython.display\n",
    "from ais_bench.infer.interface import InferSession\n",
    "\n",
    "from det_utils import letterbox, scale_coords, nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e9292fc-f49c-410d-8bf3-08069171c2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image, cfg, bgr2rgb=True):\n",
    "    \"\"\"图片预处理\"\"\"\n",
    "    img, scale_ratio, pad_size = letterbox(image, new_shape=cfg['input_shape'])\n",
    "    if bgr2rgb:\n",
    "        img = img[:, :, ::-1]\n",
    "    img = img.transpose(2, 0, 1)  # HWC2CHW\n",
    "    img = np.ascontiguousarray(img, dtype=np.float32)/255\n",
    "    return img, scale_ratio, pad_size\n",
    "\n",
    "\n",
    "def draw_bbox(bbox, img0, color, wt, names, num, a, b, c, d):\n",
    "    \"\"\"在图片上画预测框\"\"\"\n",
    "    det_result_str = ''\n",
    "    for idx, class_id in enumerate(bbox[:, 5]):\n",
    "        if float(bbox[idx][4] < float(0.05)):\n",
    "            continue\n",
    "        img0 = cv2.rectangle(img0, (int(bbox[idx][0]), int(bbox[idx][1])), (int(bbox[idx][2]), int(bbox[idx][3])),\n",
    "                             color, wt)\n",
    "        img0 = cv2.putText(img0, str(idx) + ' ' + names[int(class_id)], (int(bbox[idx][0]), int(bbox[idx][1] + 16)),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "        img0 = cv2.putText(img0, '{:.4f}'.format(bbox[idx][4]), (int(bbox[idx][0]), int(bbox[idx][1] + 32)),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "        det_result_str += '{} {} {} {} {} {}\\n'.format(\n",
    "            names[bbox[idx][5]], str(bbox[idx][4]), bbox[idx][0], bbox[idx][1], bbox[idx][2], bbox[idx][3])\n",
    "    # 在图像的右下角添加文本\n",
    "    text = f\"total:{num}  8nm:{a}  10nm:{b}  12nm:{c}  14nm:{d}\"\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.45\n",
    "    font_thickness = 1\n",
    "    font_color = (0, 0, 255)  # 红色\n",
    "    # 获取文本的大小\n",
    "    text_size = cv2.getTextSize(text, font, font_scale, font_thickness)[0]\n",
    "    # 计算文本放置的位置（右下角）\n",
    "    text_position = (10, img0.shape[0] - 10)\n",
    "    # 使用cv2.putText在图像上绘制文本\n",
    "    cv2.putText(img0, text, text_position, font, font_scale, font_color, font_thickness)\n",
    "    return img0\n",
    "\n",
    "\n",
    "def get_labels_from_txt(path):\n",
    "    \"\"\"从txt文件获取图片标签\"\"\"\n",
    "    labels_dict = dict()\n",
    "    with open(path) as f:\n",
    "        for cat_id, label in enumerate(f.readlines()):\n",
    "            labels_dict[cat_id] = label.strip()\n",
    "    return labels_dict\n",
    "\n",
    "\n",
    "def draw_prediction(pred, image, labels, num, a, b, c, d):\n",
    "    \"\"\"在图片上画出预测框并进行可视化展示\"\"\"\n",
    "    imgbox = widgets.Image(format='jpg', height=720, width=1280)\n",
    "    img_dw = draw_bbox(pred, image, (0, 255, 0), 2, labels, num, a, b, c, d)\n",
    "    imgbox.value = cv2.imencode('.jpg', img_dw)[1].tobytes()\n",
    "    display(imgbox)\n",
    "    \n",
    "def calculate_num_c(pred):\n",
    "    areas=[]\n",
    "    rows, columns = pred.shape\n",
    "    for i in range(rows):\n",
    "        area=(pred[i,2]-pred[i,0])*(pred[i,3]-pred[i,1])\n",
    "        areas.append(area)\n",
    "    if(len(areas)>=4):\n",
    "        areas_array = np.array(areas).reshape(-1, 1)  # 转换成二维数组，-1表示自动推断行数，1表示1列\n",
    "        # 使用K均值聚类，假设你希望将面积分成4个类别\n",
    "        num_clusters = 4\n",
    "        kmeans = KMeans(n_clusters=num_clusters)\n",
    "        kmeans.fit(areas_array)\n",
    "        # 获取聚类结果\n",
    "        cluster_labels = kmeans.labels_\n",
    "        centroids = kmeans.cluster_centers_\n",
    "        dic={0:int(centroids[0]),1:int(centroids[1]),2:int(centroids[2]),3:int(centroids[3])}\n",
    "        new_sys2 = sorted(dic.items(), key=lambda d: d[1], reverse=False)\n",
    "        # 打印每个框的面积和所属类别\n",
    "        cnt=[0,0,0,0]\n",
    "        for i, area in enumerate(areas):\n",
    "            if((int)(cluster_labels[i])==0):\n",
    "                cnt[0]+=1\n",
    "            elif((int)(cluster_labels[i])==1):\n",
    "                cnt[1]+=1\n",
    "            elif((int)(cluster_labels[i])==2):\n",
    "                cnt[2]+=1\n",
    "            elif((int)(cluster_labels[i])==3):\n",
    "                cnt[3]+=1\n",
    "    else:\n",
    "        return 0,0,0,0,0\n",
    "    return len(areas),cnt[new_sys2[0][0]],cnt[new_sys2[1][0]],cnt[new_sys2[2][0]],cnt[new_sys2[3][0]]\n",
    "\n",
    "def infer_image(img_path, model, class_names, cfg):\n",
    "    \"\"\"图片推理\"\"\"\n",
    "    # 图片载入\n",
    "    image = cv2.imread(img_path)\n",
    "    # 数据预处理\n",
    "    img, scale_ratio, pad_size = preprocess_image(image, cfg)\n",
    "    # 模型推理\n",
    "    output = model.infer([img])[0]\n",
    "\n",
    "    output = torch.tensor(output)\n",
    "    # 非极大值抑制后处理\n",
    "    boxout = nms(output, conf_thres=cfg[\"conf_thres\"], iou_thres=cfg[\"iou_thres\"])\n",
    "    pred_all = boxout[0].numpy()\n",
    "    # 计算小球数量\n",
    "    num,a,b,c,d=calculate_num_c(pred_all)\n",
    "    # 预测坐标转换\n",
    "    scale_coords(cfg['input_shape'], pred_all[:, :4], image.shape, ratio_pad=(scale_ratio, pad_size))\n",
    "    # 图片预测结果可视化\n",
    "    draw_prediction(pred_all, image, class_names, num, a, b, c, d)\n",
    "\n",
    "\n",
    "def infer_frame_with_vis(image, model, labels_dict, cfg, bgr2rgb=True):\n",
    "    # 数据预处理\n",
    "    img, scale_ratio, pad_size = preprocess_image(image, cfg, bgr2rgb)\n",
    "    # 模型推理\n",
    "    output = model.infer([img])[0]\n",
    "\n",
    "    output = torch.tensor(output)\n",
    "    # 非极大值抑制后处理\n",
    "    boxout = nms(output, conf_thres=cfg[\"conf_thres\"], iou_thres=cfg[\"iou_thres\"])\n",
    "    pred_all = boxout[0].numpy()\n",
    "    # 计算小球数量\n",
    "    num,a,b,c,d=calculate_num_c(pred_all)\n",
    "    # 预测坐标转换\n",
    "    scale_coords(cfg['input_shape'], pred_all[:, :4], image.shape, ratio_pad=(scale_ratio, pad_size))\n",
    "    # 图片预测结果可视化\n",
    "    img_vis = draw_bbox(pred_all, image, (0, 255, 0), 2, labels_dict, num, a, b, c, d)\n",
    "    return img_vis\n",
    "\n",
    "\n",
    "def img2bytes(image):\n",
    "    \"\"\"将图片转换为字节码\"\"\"\n",
    "    return bytes(cv2.imencode('.jpg', image)[1])\n",
    "\n",
    "\n",
    "def infer_video(video_path, model, labels_dict, cfg):\n",
    "    \"\"\"视频推理\"\"\"\n",
    "    image_widget = widgets.Image(format='jpeg', width=800, height=600)\n",
    "    display(image_widget)\n",
    "\n",
    "    # 读入视频\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while True:\n",
    "        ret, img_frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # 对视频帧进行推理\n",
    "        image_pred = infer_frame_with_vis(img_frame, model, labels_dict, cfg, bgr2rgb=True)\n",
    "        image_widget.value = img2bytes(image_pred)\n",
    "\n",
    "\n",
    "def infer_camera(model, labels_dict, cfg):\n",
    "    \"\"\"外设摄像头实时推理\"\"\"\n",
    "    def find_camera_index():\n",
    "        max_index_to_check = 10  # Maximum index to check for camera\n",
    "\n",
    "        for index in range(max_index_to_check):\n",
    "            cap = cv2.VideoCapture(index)\n",
    "            if cap.read()[0]:\n",
    "                cap.release()\n",
    "                return index\n",
    "\n",
    "        # If no camera is found\n",
    "        raise ValueError(\"No camera found.\")\n",
    "\n",
    "    # 获取摄像头\n",
    "    camera_index = find_camera_index()\n",
    "    cap = cv2.VideoCapture(camera_index)\n",
    "    # 初始化可视化对象\n",
    "    image_widget = widgets.Image(format='jpeg', width=1280, height=720)\n",
    "    display(image_widget)\n",
    "    while True:\n",
    "        # 对摄像头每一帧进行推理和可视化\n",
    "        _, img_frame = cap.read()\n",
    "        image_pred = infer_frame_with_vis(img_frame, model, labels_dict, cfg)\n",
    "        image_widget.value = img2bytes(image_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e455e8d",
   "metadata": {},
   "source": [
    "# 样例运行\n",
    "\n",
    "* 初始化相关参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dccded13-fad1-4b88-b63f-d46deb23aa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'conf_thres': 0.2,  # 模型置信度阈值，阈值越低，得到的预测框越多\n",
    "    'iou_thres': 0.3,  # IOU阈值，高于这个阈值的重叠预测框会被过滤掉\n",
    "    'input_shape': [640, 640],  # 模型输入尺寸\n",
    "}\n",
    "\n",
    "model_path = 'best.om'\n",
    "label_path = './test.txt'\n",
    "# 初始化推理模型\n",
    "model = InferSession(0, model_path)\n",
    "labels_dict = get_labels_from_txt(label_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e9cf34",
   "metadata": {},
   "source": [
    "* 选择推理模式。\"infer_mode\"有三个取值：image, camera, video，分别对应图片推理、摄像头实时推理和视频推理。默认使用视频推理模式。\n",
    "* 我们选取的样例是一个赛车视频，执行下面的代码后可以看到模型会对视频的每一帧进行推理，并将预测结果展示在画面上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab68b6b-7d65-4dd9-ab0b-c5c9733c415a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] acl init success\n",
      "[INFO] open device 0 success\n",
      "[INFO] load model best.om success\n",
      "[INFO] create model description success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@8.372] global cap_v4l.cpp:982 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n",
      "[ERROR:0@8.557] global obsensor_uvc_stream_channel.cpp:156 getStreamChannelGroup Camera index out of range\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0213f2ceb0fd425aad103ace240f9dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', format='jpeg', height='720', width='1280')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "infer_mode = 'camera'\n",
    "\n",
    "if infer_mode == 'image':\n",
    "    img_path = 'test2.jpg'\n",
    "    infer_image(img_path, model, labels_dict, cfg)\n",
    "elif infer_mode == 'camera':\n",
    "    infer_camera(model, labels_dict, cfg)\n",
    "elif infer_mode == 'video':\n",
    "    video_path = 'racing.mp4'\n",
    "    infer_video(video_path, model, labels_dict, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27887b77",
   "metadata": {},
   "source": [
    "# 样例总结与扩展\n",
    "以上就是这个样例的全部内容了，值得关注的是在模型推理后有一步非常重要的后处理，就是非极大值抑制，即NMS，由于模型的原始预测结果会有非常多无效或重叠的预测框，我们需要通过NMS来进行过滤。再者，模型预测框的表示往往是一个标准化的结果，比如0到1之间，我们需要通过坐标转换将结果与原始图片的宽高对应上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2f1652-8258-454f-a060-ee4e0d700ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd3763c-adeb-417a-91fc-e66a141a4e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41831f0f-d749-4806-bf41-432d909eb226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
